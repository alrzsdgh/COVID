{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown\n!gdown --id 1aUTHbMopudfj_ATmmp_DNjr2BxX9M1c8\n!gdown --id 1X5Ek5mQYjNkw8k87gt_VCFDhPCSwXbmF\n!gdown --id 1q_T2QobAT3EGdiDIuZ35qIGsDY3ynzLg","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-02T13:48:12.993008Z","iopub.execute_input":"2023-10-02T13:48:12.993909Z","iopub.status.idle":"2023-10-02T13:49:29.194555Z","shell.execute_reply.started":"2023-10-02T13:48:12.993875Z","shell.execute_reply":"2023-10-02T13:49:29.193493Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1aUTHbMopudfj_ATmmp_DNjr2BxX9M1c8\nFrom (redirected): https://drive.google.com/uc?id=1aUTHbMopudfj_ATmmp_DNjr2BxX9M1c8&confirm=t&uuid=74e45c99-c0e9-4790-9396-b7a03c8f10b7\nTo: /kaggle/working/3D-IR-MAH-CT.zip\n100%|███████████████████████████████████████| 2.73G/2.73G [00:21<00:00, 127MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1X5Ek5mQYjNkw8k87gt_VCFDhPCSwXbmF\nFrom (redirected): https://drive.google.com/uc?id=1X5Ek5mQYjNkw8k87gt_VCFDhPCSwXbmF&confirm=t&uuid=e50c7d88-7875-4ab8-822e-4d99f0ed1e7d\nTo: /kaggle/working/NL3DTIF.zip\n100%|██████████████████████████████████████| 2.94G/2.94G [00:32<00:00, 91.1MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1q_T2QobAT3EGdiDIuZ35qIGsDY3ynzLg\nTo: /kaggle/working/MASERes_3.h5\n100%|██████████████████████████████████████| 29.2M/29.2M [00:00<00:00, 65.1MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip '/kaggle/working/NL3DTIF.zip' -d '/kaggle/working/'\n!unzip '/kaggle/working/3D-IR-MAH-CT.zip' -d '/kaggle/working/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport zipfile\nimport numpy as np\nimport tensorflow as tf\nimport random\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nfrom scipy import ndimage","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:51:52.848736Z","iopub.execute_input":"2023-10-02T13:51:52.849075Z","iopub.status.idle":"2023-10-02T13:52:00.298550Z","shell.execute_reply.started":"2023-10-02T13:51:52.849049Z","shell.execute_reply":"2023-10-02T13:52:00.297671Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"os.remove('/kaggle/working/3D-IR-MAH-CT.zip')\nos.remove('/kaggle/working/NL3DTIF.zip')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:52:00.300351Z","iopub.execute_input":"2023-10-02T13:52:00.301200Z","iopub.status.idle":"2023-10-02T13:52:00.744585Z","shell.execute_reply.started":"2023-10-02T13:52:00.301166Z","shell.execute_reply":"2023-10-02T13:52:00.740125Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def resize_volume(img):\n    \"\"\"Resize across z-axis\"\"\"\n    # Set the desired depth\n    desired_depth = 128\n    desired_width = 128\n    desired_height = 128\n    # Get current depth\n    current_depth = img.shape[0]\n    current_width = img.shape[1]\n    current_height = img.shape[2]\n    # Compute depth factor\n    depth = current_depth / desired_depth\n    width = current_width / desired_width\n    height = current_height / desired_height\n    depth_factor = 1 / depth\n    width_factor = 1 / width\n    height_factor = 1 / height\n    # Resize across z-axis\n    img = ndimage.zoom(img, (depth_factor, width_factor, height_factor, 1))\n    return img\n\n\n\n\n\n\ndef plot_slices(num_rows, num_cols, data):\n    # create figure\n    fig = plt.figure(figsize=(10, 7))\n    \n    n = len(data)\n  \n    for i in range(num_rows * num_cols):\n        if i < n:\n            # Adds a subplot at the 1st position\n            fig.add_subplot(num_rows, num_cols, i+1)\n\n            # showing image\n            plt.imshow(data[i])\n            plt.axis('off')\n            \n            \n      \n    \n    \ndef prepare_3D_samples(main_dir, inp_lst):\n    \n    final_lst = []\n    for i in tqdm(inp_lst):\n        tmp_dir = os.path.join(main_dir,i)\n        tmp_lst = os.listdir(tmp_dir)\n        tmp_lst.sort()\n        \n        tmp_3d = []\n        for j in tmp_lst:\n            tmp_3d.append(plt.imread(os.path.join(main_dir,i,j)))\n        \n        \n        final_lst.append(resize_volume(np.array(tmp_3d)))\n    \n    return np.array(final_lst)\n\n\n\n\ndef rotate(volume):\n    \"\"\"Rotate the volume by a few degrees\"\"\"\n\n    def scipy_rotate(volume):\n        # define some rotation angles\n        angles = [-20, -10, -5, 0, 5, 10, 20]\n        # pick angles at random\n        angle = random.choice(angles)\n        # rotate volume\n        volume = ndimage.rotate(volume, angle, reshape=False)\n        return volume\n    \n    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n    return augmented_volume\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:52:32.665802Z","iopub.execute_input":"2023-10-02T13:52:32.666123Z","iopub.status.idle":"2023-10-02T13:52:32.678373Z","shell.execute_reply.started":"2023-10-02T13:52:32.666098Z","shell.execute_reply":"2023-10-02T13:52:32.677447Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cov_main_dir = '/kaggle/working/covid_3D'\nnorm_main_dir = '/kaggle/working/NL3DTIF'\n\ncov_lst = os.listdir(cov_main_dir)\ncov_lst.sort()\nrandom.Random(1).shuffle(cov_lst)\n\nnorm_lst = os.listdir(norm_main_dir)\nnorm_lst.sort()\nrandom.Random(1).shuffle(norm_lst)\n\nprint(len(cov_lst))\nprint(cov_lst[:5])\nprint(len(norm_lst))\nprint(norm_lst[:5])\nprint('_________')\n\ncovid_volume = prepare_3D_samples(cov_main_dir, cov_lst)\nnormal_volume = prepare_3D_samples(norm_main_dir, norm_lst)\nprint('_________')\n\nprint('covid_volume:', np.shape(covid_volume))\nprint('normal_volume:', np.shape(normal_volume))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T13:52:34.706688Z","iopub.execute_input":"2023-10-02T13:52:34.707015Z","iopub.status.idle":"2023-10-02T15:23:47.767682Z","shell.execute_reply.started":"2023-10-02T13:52:34.706992Z","shell.execute_reply":"2023-10-02T15:23:47.766697Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"133\n['77', '84', '65', '61', '97']\n76\n['020', '067', '003', '057', '012']\n_________\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 133/133 [53:47<00:00, 24.26s/it]\n100%|██████████| 76/76 [37:25<00:00, 29.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"_________\ncovid_volume: (133, 128, 128, 128, 3)\nnormal_volume: (76, 128, 128, 128, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"normal_labels = np.array([1 for _ in range(len(normal_volume))])\ncovid_labels = np.array([0 for _ in range(len(covid_volume))])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:48:09.779841Z","iopub.execute_input":"2023-10-02T15:48:09.780183Z","iopub.status.idle":"2023-10-02T15:48:09.784967Z","shell.execute_reply.started":"2023-10-02T15:48:09.780155Z","shell.execute_reply":"2023-10-02T15:48:09.783940Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"x_train = np.concatenate((covid_volume[:107], normal_volume[:61]), axis=0)\ny_train = np.concatenate((covid_labels[:107], normal_labels[:61]), axis=0)\n\nx_valid = np.concatenate((covid_volume[107:120], normal_volume[61:68]), axis=0)\ny_valid = np.concatenate((covid_labels[107:120], normal_labels[61:68]), axis=0)\n\nx_test = np.concatenate((covid_volume[120:], normal_volume[68:]), axis=0)\ny_test = np.concatenate((covid_labels[120:], normal_labels[68:]), axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:48:11.197697Z","iopub.execute_input":"2023-10-02T15:48:11.198039Z","iopub.status.idle":"2023-10-02T15:48:12.625147Z","shell.execute_reply.started":"2023-10-02T15:48:11.198016Z","shell.execute_reply":"2023-10-02T15:48:12.623950Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print('x_train:', np.shape(x_train))\nprint('y_train:', np.shape(y_train))\nprint('x_valid:', np.shape(x_valid))\nprint('y_valid:', np.shape(y_valid))\nprint('x_test:', np.shape(x_test))\nprint('y_test:', np.shape(y_test))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:48:13.956551Z","iopub.execute_input":"2023-10-02T15:48:13.957512Z","iopub.status.idle":"2023-10-02T15:48:13.963924Z","shell.execute_reply.started":"2023-10-02T15:48:13.957472Z","shell.execute_reply":"2023-10-02T15:48:13.962763Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"x_train: (168, 128, 128, 128, 3)\ny_train: (168,)\nx_valid: (20, 128, 128, 128, 3)\ny_valid: (20,)\nx_test: (21, 128, 128, 128, 3)\ny_test: (21,)\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_preprocessing(volume, label):\n    # rescaling\n    volume = tf.cast(volume, tf.float32) / 255.0\n    # Rotate volume\n    volume = rotate(volume)\n    return volume, label\n\n\ndef validation_preprocessing(volume, label):\n    # rescaling\n    volume = tf.cast(volume, tf.float32) / 255.0\n    return volume, label","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:48:15.642222Z","iopub.execute_input":"2023-10-02T15:48:15.642536Z","iopub.status.idle":"2023-10-02T15:48:15.648062Z","shell.execute_reply.started":"2023-10-02T15:48:15.642510Z","shell.execute_reply":"2023-10-02T15:48:15.647173Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define data loaders.\ntrain_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nvalidation_loader = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\ntest_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n\nbatch_size = 2\n\n# Augment the on the fly during training.\ntrain_dataset = (\n    train_loader.shuffle(len(x_train))\n    .map(train_preprocessing)\n    .batch(batch_size)\n    .prefetch(1)\n)\n\n# Only rescale.\nvalidation_dataset = (\n    validation_loader.shuffle(len(x_valid))\n    .map(validation_preprocessing)\n    .batch(batch_size)\n    .prefetch(1)\n)\n\n# Only rescale.\ntest_dataset = (\n    test_loader.shuffle(len(x_test))\n    .map(validation_preprocessing)\n    .batch(batch_size)\n    .prefetch(1)\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:48:16.996552Z","iopub.execute_input":"2023-10-02T15:48:16.997351Z","iopub.status.idle":"2023-10-02T15:48:23.862160Z","shell.execute_reply.started":"2023-10-02T15:48:16.997314Z","shell.execute_reply":"2023-10-02T15:48:23.861255Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# ablation","metadata":{}},{"cell_type":"code","source":"def maseres():\n    model = tf.keras.models.load_model('/kaggle/working/MASERes_3.h5')\n    return model\n\ndef ablation(inp_model, inp_layer, inp_filters):\n    # inp_model: the model on which ablation is applied\n    # inp_layer: an integer that shows the layer's order (n: n'th layer)\n    # inp_filter: a list of integers that shows the filter's\n    #                     order in the layer([a,b,c]: a'th, b'th, and c'th filters)\n    if len(inp_model.layers[inp_layer].get_weights())  == 2:\n        w = inp_model.layers[inp_layer].get_weights()[0]\n        b = inp_model.layers[inp_layer].get_weights()[1]\n        for i in inp_filters:\n            w[..., i] = np.zeros((np.shape(w[... ,i])))\n            b[i] = 0\n        inp_model.layers[inp_layer].set_weights([w,b])\n      \n    \n    elif len(inp_model.layers[inp_layer].get_weights()) == 1:\n        w = inp_model.layers[inp_layer].get_weights()[0]\n        for i in inp_filters:\n                w[..., i] = np.zeros((np.shape(w[... ,i])))\n        inp_model.layers[inp_layer].set_weights([w])\n    \n    return inp_model\n\n\ndef ablation_attention(inp_model, inp_layer, inp_neurons):\n    \n    w = inp_model.layers[inp_layer].get_weights()[0]\n    b = inp_model.layers[inp_layer].get_weights()[1]\n\n    for i in inp_neurons:\n        w[i] = np.zeros((np.shape(w[i])))\n    inp_model.layers[inp_layer].set_weights([w, b])\n    \n    return inp_model\n\n\ndef results(inp_model, inp_data):\n    inp_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6) , \n                loss='binary_crossentropy', metrics = ['acc',\n                                                      tf.keras.metrics.AUC(),\n                                                      tf.keras.metrics.Precision(),\n                                                      tf.keras.metrics.Recall(),\n                                                      tf.keras.metrics.TruePositives(),\n                                                      tf.keras.metrics.TrueNegatives(),\n                                                      tf.keras.metrics.FalsePositives(),\n                                                      tf.keras.metrics.FalseNegatives()])\n    \n    a = inp_model.evaluate(inp_data)\n\n    conf_mx = [[a[6], a[7]],[a[8], a[5]]]\n    mx = np.array(conf_mx)\n    fscore = 2*a[3]*a[4]/(a[3]+a[4])\n    spc = mx[0, 0] * 1.0 / (mx[0, 0] + mx[0, 1])\n    sen = mx[1,1] * 1.0 / (mx[1,1] + mx[1,0])\n\n    print('_________')\n    print('accuracy:',np.round(a[1]*100,2),'%')\n    print('precision:',np.round(a[3]*100,2),'%')\n    print('recall:',np.round(a[4]*100,2),'%')\n    print('Sensitivity:',np.round(sen*100,2),'%')\n    print('Specificity:',np.round(spc*100,2),'%')\n    print('f1-score:',np.round(fscore*100,2),'%')\n    print('AUC:',np.round(a[2]*100,2),'%')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:48:42.015036Z","iopub.execute_input":"2023-10-02T15:48:42.015381Z","iopub.status.idle":"2023-10-02T15:48:42.029328Z","shell.execute_reply.started":"2023-10-02T15:48:42.015354Z","shell.execute_reply":"2023-10-02T15:48:42.027962Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# main model's layers\n\nmain_model = maseres()\nfor i, j in enumerate(main_model.layers):\n    print('idx', i, ':', j)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:48:45.494485Z","iopub.execute_input":"2023-10-02T15:48:45.494818Z","iopub.status.idle":"2023-10-02T15:48:46.655395Z","shell.execute_reply.started":"2023-10-02T15:48:45.494793Z","shell.execute_reply":"2023-10-02T15:48:46.654469Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"idx 0 : <keras.engine.input_layer.InputLayer object at 0x7e6096bb3970>\nidx 1 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e6096bb22f0>\nidx 2 : <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7e6096bb3460>\nidx 3 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e6096bb3070>\nidx 4 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e6096b99d50>\nidx 5 : <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7e6096b99ae0>\nidx 6 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e6096b99510>\nidx 7 : <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7e6096b99000>\nidx 8 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e6096b98d30>\nidx 9 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e6096b98880>\nidx 10 : <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7e6096b983d0>\nidx 11 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e6096b98610>\nidx 12 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e6096b970d0>\nidx 13 : <keras.layers.pooling.global_average_pooling3d.GlobalAveragePooling3D object at 0x7e6096b96a10>\nidx 14 : <keras.layers.core.dense.Dense object at 0x7e6096b96350>\nidx 15 : <keras.layers.core.dense.Dense object at 0x7e6096b96200>\nidx 16 : <keras.layers.merging.multiply.Multiply object at 0x7e6096b97f70>\nidx 17 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e6096b97fa0>\nidx 18 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e6096b976d0>\nidx 19 : <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7e6096b95f60>\nidx 20 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e6096b95c30>\nidx 21 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e6096b89660>\nidx 22 : <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7e6096b89330>\nidx 23 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e6096b88250>\nidx 24 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e6096b88760>\nidx 25 : <keras.layers.pooling.global_average_pooling3d.GlobalAveragePooling3D object at 0x7e6096b88a90>\nidx 26 : <keras.layers.core.dense.Dense object at 0x7e6096c559f0>\nidx 27 : <keras.layers.core.dense.Dense object at 0x7e6096c56bc0>\nidx 28 : <keras.layers.merging.multiply.Multiply object at 0x7e6096c55090>\nidx 29 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e6096c54fd0>\nidx 30 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e6096c568f0>\nidx 31 : <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7e602c4ad690>\nidx 32 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e602c4af700>\nidx 33 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e602c4af430>\nidx 34 : <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7e602c4ad000>\nidx 35 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e602c4afb80>\nidx 36 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e602c4db160>\nidx 37 : <keras.layers.pooling.global_average_pooling3d.GlobalAveragePooling3D object at 0x7e602c4d8550>\nidx 38 : <keras.layers.core.dense.Dense object at 0x7e6096c461a0>\nidx 39 : <keras.layers.core.dense.Dense object at 0x7e6096c46e90>\nidx 40 : <keras.layers.merging.multiply.Multiply object at 0x7e6096c45cf0>\nidx 41 : <keras.layers.convolutional.conv3d.Conv3D object at 0x7e6096c45300>\nidx 42 : <keras.layers.core.tf_op_layer.TFOpLambda object at 0x7e6096c46140>\nidx 43 : <keras.layers.pooling.max_pooling3d.MaxPooling3D object at 0x7e602e699e10>\nidx 44 : <keras.layers.core.dense.Dense object at 0x7e6096bba050>\nidx 45 : <keras.layers.core.dense.Dense object at 0x7e6096bb9ea0>\nidx 46 : <keras.layers.core.dense.Dense object at 0x7e6096bba590>\nidx 47 : <keras.layers.core.dense.Dense object at 0x7e6096bb97e0>\nidx 48 : <keras.layers.core.dense.Dense object at 0x7e6096bb93c0>\nidx 49 : <keras.layers.core.dense.Dense object at 0x7e6096bb8fa0>\nidx 50 : <keras.layers.merging.multiply.Multiply object at 0x7e6096bb8340>\nidx 51 : <keras.layers.merging.multiply.Multiply object at 0x7e6096bb8400>\nidx 52 : <keras.layers.merging.multiply.Multiply object at 0x7e6096bb86a0>\nidx 53 : <keras.layers.core.activation.Activation object at 0x7e6096bb8940>\nidx 54 : <keras.layers.core.dense.Dense object at 0x7e6096bb8af0>\nidx 55 : <keras.layers.core.activation.Activation object at 0x7e6096bbac50>\nidx 56 : <keras.layers.core.dense.Dense object at 0x7e6096bbac20>\nidx 57 : <keras.layers.core.activation.Activation object at 0x7e60969ef580>\nidx 58 : <keras.layers.core.dense.Dense object at 0x7e60969ef5b0>\nidx 59 : <keras.layers.merging.multiply.Multiply object at 0x7e60969efc10>\nidx 60 : <keras.layers.merging.multiply.Multiply object at 0x7e60969efcd0>\nidx 61 : <keras.layers.merging.multiply.Multiply object at 0x7e6096a000a0>\nidx 62 : <keras.layers.merging.concatenate.Concatenate object at 0x7e6096a00250>\nidx 63 : <keras.layers.reshaping.flatten.Flatten object at 0x7e6096a006a0>\nidx 64 : <keras.layers.core.dense.Dense object at 0x7e6096bb3b50>\nidx 65 : <keras.layers.core.dense.Dense object at 0x7e6096a00b80>\nidx 66 : <keras.layers.regularization.dropout.Dropout object at 0x7e6096a00fa0>\nidx 67 : <keras.layers.core.dense.Dense object at 0x7e6096a01540>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Damaged Model I\n\nmain_model = maseres()\n\n# index of filters to ablate from layer idx1 (selecting 4 out of 16)\ntmp_lst1 = list(range(16))\nrandom.Random(1).shuffle(tmp_lst1)\nidx1_ablated_filters = tmp_lst1[:4]\n\n# index of filters to ablate from layer idx4 (selecting 4 out of 16)\ntmp_lst2 = list(range(16))\nrandom.Random(2).shuffle(tmp_lst2)\nidx2_ablated_filters = tmp_lst2[:4]\n\n# ablation in layer idx1\ntmp_dmg_i_model = ablation(main_model, inp_layer = 1, inp_filters = idx1_ablated_filters)\ndamaged_model_i = ablation(tmp_dmg_i_model, inp_layer = 4,\n                           inp_filters = idx2_ablated_filters)\n\nresults(damaged_model_i, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:49:05.810578Z","iopub.execute_input":"2023-10-02T15:49:05.811270Z","iopub.status.idle":"2023-10-02T15:49:20.863956Z","shell.execute_reply.started":"2023-10-02T15:49:05.811242Z","shell.execute_reply":"2023-10-02T15:49:20.863030Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"11/11 [==============================] - 14s 446ms/step - loss: 0.8562 - acc: 0.4762 - auc: 0.9904 - precision: 0.4211 - recall: 1.0000 - true_positives: 8.0000 - true_negatives: 2.0000 - false_positives: 11.0000 - false_negatives: 0.0000e+00\n_________\naccuracy: 47.62 %\nprecision: 42.11 %\nrecall: 100.0 %\nSensitivity: 100.0 %\nSpecificity: 15.38 %\nf1-score: 59.26 %\nAUC: 99.04 %\n","output_type":"stream"}]},{"cell_type":"code","source":"# Damaged Model II\n\nmain_model = maseres()\n\nlayers_lst = [9,12,14,15,17,21,24,26,27,29,33,36,38,39,41]\nidx = dict()\nfor i, lyr in enumerate(layers_lst):\n    n_filters = np.shape(main_model.layers[lyr].get_weights()[0])\n    idx[f'{lyr}'] = list(range(n_filters[-1]))\n    random.Random(i+1).shuffle(idx[f'{lyr}'])\n    idx[f'{lyr}'] = idx[f'{lyr}'][:int(0.25*n_filters[-1])]\n    \nfor k,v in idx.items():    \n    damaged_model_ii = ablation(main_model, inp_layer = int(k), inp_filters = v)\n    \nresults(damaged_model_ii, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:49:37.122519Z","iopub.execute_input":"2023-10-02T15:49:37.122922Z","iopub.status.idle":"2023-10-02T15:49:42.995393Z","shell.execute_reply.started":"2023-10-02T15:49:37.122891Z","shell.execute_reply":"2023-10-02T15:49:42.994365Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"11/11 [==============================] - 5s 330ms/step - loss: 0.3380 - acc: 0.8571 - auc_1: 0.9712 - precision_1: 1.0000 - recall_1: 0.6250 - true_positives_1: 5.0000 - true_negatives_1: 13.0000 - false_positives_1: 0.0000e+00 - false_negatives_1: 3.0000\n_________\naccuracy: 85.71 %\nprecision: 100.0 %\nrecall: 62.5 %\nSensitivity: 62.5 %\nSpecificity: 100.0 %\nf1-score: 76.92 %\nAUC: 97.12 %\n","output_type":"stream"}]},{"cell_type":"code","source":"# Damaged Model III\n\nmain_model = maseres()\n\nlayers_lst = [44,45,46,47,48,49,54,56,58]\nidx = dict()\nfor i, lyr in enumerate(layers_lst):\n    n_neuron = np.shape(main_model.layers[lyr].get_weights()[0])\n    idx[f'{lyr}'] = list(range(n_neuron[0]))\n    random.Random(i+1).shuffle(idx[f'{lyr}'])\n    idx[f'{lyr}'] = idx[f'{lyr}'][:int(0.25*n_neuron[0])]\n    \nfor k,v in idx.items():    \n    damaged_model_iii = ablation_attention(main_model, inp_layer = int(k), inp_neurons = v)\n    \ndamaged_model_iii.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6) , \n                loss='binary_crossentropy', metrics = ['acc',\n                                                      tf.keras.metrics.AUC(),\n                                                      tf.keras.metrics.Precision(),\n                                                      tf.keras.metrics.Recall(),\n                                                      tf.keras.metrics.TruePositives(),\n                                                      tf.keras.metrics.TrueNegatives(),\n                                                      tf.keras.metrics.FalsePositives(),\n                                                      tf.keras.metrics.FalseNegatives()])\n\ndamaged_model_iii.evaluate(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:51:47.523334Z","iopub.execute_input":"2023-10-02T15:51:47.523691Z","iopub.status.idle":"2023-10-02T15:51:54.805588Z","shell.execute_reply.started":"2023-10-02T15:51:47.523660Z","shell.execute_reply":"2023-10-02T15:51:54.804577Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"11/11 [==============================] - 5s 328ms/step - loss: 1.9398 - acc: 0.6190 - auc_5: 0.7500 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00 - true_positives_5: 0.0000e+00 - true_negatives_5: 13.0000 - false_positives_5: 0.0000e+00 - false_negatives_5: 8.0000\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[1.9398425817489624, 0.6190476417541504, 0.75, 0.0, 0.0, 0.0, 13.0, 0.0, 8.0]"},"metadata":{}}]},{"cell_type":"code","source":"# Damaged Model IV\n\nmain_model = maseres()\n\nlayers_lst = [64]\nidx = dict()\nfor i, lyr in enumerate(layers_lst):\n    n_filters = np.shape(main_model.layers[lyr].get_weights()[0])\n    idx[f'{lyr}'] = list(range(n_filters[-1]))\n    random.Random(i+1).shuffle(idx[f'{lyr}'])\n    idx[f'{lyr}'] = idx[f'{lyr}'][:int(0.25*n_filters[-1])]\n    \nfor k,v in idx.items():    \n    damaged_model_iv = ablation(main_model, inp_layer = int(k), inp_filters = v)\n    \nresults(damaged_model_iv, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:50:03.347167Z","iopub.execute_input":"2023-10-02T15:50:03.347486Z","iopub.status.idle":"2023-10-02T15:50:09.059368Z","shell.execute_reply.started":"2023-10-02T15:50:03.347460Z","shell.execute_reply":"2023-10-02T15:50:09.058346Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"11/11 [==============================] - 5s 329ms/step - loss: 0.1465 - acc: 0.9524 - auc_3: 1.0000 - precision_3: 1.0000 - recall_3: 0.8750 - true_positives_3: 7.0000 - true_negatives_3: 13.0000 - false_positives_3: 0.0000e+00 - false_negatives_3: 1.0000\n_________\naccuracy: 95.24 %\nprecision: 100.0 %\nrecall: 87.5 %\nSensitivity: 87.5 %\nSpecificity: 100.0 %\nf1-score: 93.33 %\nAUC: 100.0 %\n","output_type":"stream"}]},{"cell_type":"code","source":"# Damaged Model V\n\nmain_model = maseres()\n\nlayers_lst = [65]\nidx = dict()\nfor i, lyr in enumerate(layers_lst):\n    n_filters = np.shape(main_model.layers[lyr].get_weights()[0])\n    idx[f'{lyr}'] = list(range(n_filters[-1]))\n    random.Random(i+1).shuffle(idx[f'{lyr}'])\n    idx[f'{lyr}'] = idx[f'{lyr}'][:int(0.25*n_filters[-1])]\n    \nfor k,v in idx.items():    \n    damaged_model_v = ablation(main_model, inp_layer = int(k), inp_filters = v)\n    \nresults(damaged_model_v, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T15:50:12.759556Z","iopub.execute_input":"2023-10-02T15:50:12.760599Z","iopub.status.idle":"2023-10-02T15:50:19.944329Z","shell.execute_reply.started":"2023-10-02T15:50:12.760552Z","shell.execute_reply":"2023-10-02T15:50:19.943231Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"11/11 [==============================] - 5s 329ms/step - loss: 0.0807 - acc: 1.0000 - auc_4: 1.0000 - precision_4: 1.0000 - recall_4: 1.0000 - true_positives_4: 8.0000 - true_negatives_4: 13.0000 - false_positives_4: 0.0000e+00 - false_negatives_4: 0.0000e+00\n_________\naccuracy: 100.0 %\nprecision: 100.0 %\nrecall: 100.0 %\nSensitivity: 100.0 %\nSpecificity: 100.0 %\nf1-score: 100.0 %\nAUC: 100.0 %\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}